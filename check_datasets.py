#!/usr/bin/env python3
"""
Script kiá»ƒm tra tráº¡ng thÃ¡i vÃ  thá»‘ng kÃª cÃ¡c dataset cÃ³ sáºµn
"""

import os
from datetime import datetime

def format_size(size_bytes):
    """Format file size"""
    if size_bytes == 0:
        return "0B"
    size_names = ["B", "KB", "MB", "GB"]
    i = 0
    while size_bytes >= 1024 and i < len(size_names)-1:
        size_bytes /= 1024.0
        i += 1
    return f"{size_bytes:.1f}{size_names[i]}"

def count_lines(file_path):
    """Äáº¿m sá»‘ dÃ²ng trong file"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return sum(1 for _ in f)
    except:
        return 0

def check_dataset_file(file_path, description):
    """Kiá»ƒm tra má»™t file dataset"""
    if os.path.exists(file_path):
        size = os.path.getsize(file_path)
        lines = count_lines(file_path)
        mod_time = datetime.fromtimestamp(os.path.getmtime(file_path))
        
        print(f"âœ… {description}")
        print(f"   ğŸ“ {file_path}")
        print(f"   ğŸ“Š Size: {format_size(size)}")
        print(f"   ğŸ“„ Lines: {lines:,}")
        print(f"   ğŸ• Modified: {mod_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # Show sample if not too large
        if size < 100 * 1024 * 1024:  # < 100MB
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    sample = f.readline().strip()
                    if '\t' in sample:
                        x_raw, y_gold = sample.split('\t', 1)
                        print(f"   ğŸ“ Sample: '{x_raw[:50]}...' -> '{y_gold[:50]}...'")
            except:
                pass
        print()
        
        return True, lines, size
    else:
        print(f"âŒ {description}")
        print(f"   ğŸ“ {file_path} (khÃ´ng tá»“n táº¡i)")
        print()
        return False, 0, 0

def main():
    print("ğŸ” KIá»‚M TRA TRáº NG THÃI DATASETS")
    print("=" * 60)
    
    datasets = [
        # Original datasets
        ("data/Viet74K_clean.txt", "Viet74K Clean Corpus"),
        ("data/train.txt", "Small Train Set"),
        ("data/dev.txt", "Small Dev Set"), 
        ("data/test.txt", "Small Test Set"),
        
        # Large corpus files
        ("data/corpus-full.txt", "Original Large Corpus"),
        ("data/corpus_full_processed.txt", "Processed Large Corpus"),
        ("data/corpus_full_processed_train.txt", "Large Train Set"),
        ("data/corpus_full_processed_dev.txt", "Large Dev Set"),
        ("data/corpus_full_processed_test.txt", "Large Test Set"),
        
        # Sample files
        ("data/corpus_sample.txt", "Sample Corpus"),
        ("data/corpus_sample_processed.txt", "Processed Sample"),
    ]
    
    total_files = 0
    total_lines = 0
    total_size = 0
    
    for file_path, description in datasets:
        exists, lines, size = check_dataset_file(file_path, description)
        if exists:
            total_files += 1
            total_lines += lines
            total_size += size
    
    print("=" * 60)
    print("ğŸ“Š Tá»”NG Káº¾T")
    print("=" * 60)
    print(f"âœ… Files cÃ³ sáºµn: {total_files}/{len(datasets)}")
    print(f"ğŸ“„ Tá»•ng lines: {total_lines:,}")
    print(f"ğŸ’¾ Tá»•ng size: {format_size(total_size)}")
    
    # Recommendations
    print("\nğŸ¯ KHUYáº¾N NGHá»Š")
    print("=" * 60)
    
    large_train_exists = os.path.exists("data/corpus_full_processed_train.txt")
    large_dev_exists = os.path.exists("data/corpus_full_processed_dev.txt")
    large_test_exists = os.path.exists("data/corpus_full_processed_test.txt")
    
    if large_train_exists and large_dev_exists and large_test_exists:
        print("ğŸš€ Sáº´N SÃ€NG TRAINING Vá»šI LARGE CORPUS!")
        print("\nğŸ“‹ CÃ¡c lá»‡nh training khuyáº¿n nghá»‹:")
        print("   ğŸ§ª Test nhanh (10K samples):")
        print("      python train_large_corpus.py --test-run")
        print()
        print("   ğŸ“Š Training trung bÃ¬nh (100K samples):")
        print("      python train_large_corpus.py --samples 100000")
        print()
        print("   ğŸš€ Training lá»›n (500K samples):")
        print("      python train_large_corpus.py --samples 500000")
        print()
        print("   ğŸ¯ Káº¿t há»£p táº¥t cáº£ datasets (Viet74K + corpus-full.txt):")
        print("      python train_large_corpus.py --combine-all --samples 1000000")
        print()
        print("   ğŸ† TRAINING TOÃ€N Bá»˜ (táº¥t cáº£ dá»¯ liá»‡u cÃ³ sáºµn):")
        print("      python train_large_corpus.py --full-training")
        
    elif os.path.exists("data/corpus-full.txt"):
        print("ğŸ“ˆ CÃ“ LARGE CORPUS NHÆ¯NG CHÆ¯A Xá»¬ LÃ")
        print("\nğŸ’¡ Cháº¡y lá»‡nh sau Ä‘á»ƒ xá»­ lÃ½:")
        print("   python src/process_full_corpus_background.py")
        
    else:
        print("ğŸ“š CHá»ˆ CÃ“ SMALL CORPUS")
        print("\nğŸ’¡ CÃ³ thá»ƒ training vá»›i:")
        print("   python src/training.py")
    
    # Model status
    print("\nğŸ¤– TRáº NG THÃI MODELS")
    print("=" * 60)
    
    model_dirs = [
        ("models/crf", "CRF Model (Small Corpus)"),
        ("models/crf_large", "CRF Model (Large Corpus)"),
        ("models/crf_test", "CRF Test Model"),
    ]
    
    for model_dir, description in model_dirs:
        model_path = os.path.join(model_dir, "best_model.pkl")
        metadata_path = os.path.join(model_dir, "best_model_metadata.json")
        
        if os.path.exists(model_path):
            print(f"âœ… {description}")
            print(f"   ğŸ“ {model_path}")
            
            if os.path.exists(metadata_path):
                try:
                    import json
                    with open(metadata_path, 'r', encoding='utf-8') as f:
                        metadata = json.load(f)
                    
                    print(f"   ğŸ“Š F1-score: {metadata.get('test_f1', 'N/A')}")
                    print(f"   ğŸ“š Train size: {metadata.get('train_size', 'N/A'):,}")
                    print(f"   ğŸ”§ Data source: {metadata.get('data_source', 'N/A')}")
                except:
                    pass
        else:
            print(f"âŒ {description} (chÆ°a train)")
        print()

if __name__ == "__main__":
    main() 
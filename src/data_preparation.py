"""
BÆ°á»›c 1: Chuáº©n bá»‹ Dá»¯ liá»‡u
- Thu tháº­p corpus Vietnamese Ä‘Ã£ annotate 
- Tiá»n xá»­ lÃ½: lowercase, loáº¡i bá» dáº¥u, xÃ³a space â†’ táº¡o X_raw
- Giá»¯ báº£n chuáº©n (cÃ³ dáº¥u + khoáº£ng tráº¯ng) cho nhÃ£n Y_gold
- Táº¡o bá»™ train/dev/test
- Há»— trá»£ xá»­ lÃ½ file lá»›n vá»›i streaming
"""

import re
import unicodedata
import random
from typing import List, Tuple, Dict, Iterator
import pandas as pd
from sklearn.model_selection import train_test_split
import os

class VietnameseDataPreprocessor:
    def __init__(self):
        # Mapping cÃ¡c kÃ½ tá»± cÃ³ dáº¥u vá» khÃ´ng dáº¥u
        self.diacritic_map = {
            'Ã ': 'a', 'Ã¡': 'a', 'áº£': 'a', 'Ã£': 'a', 'áº¡': 'a',
            'Äƒ': 'a', 'áº±': 'a', 'áº¯': 'a', 'áº³': 'a', 'áºµ': 'a', 'áº·': 'a',
            'Ã¢': 'a', 'áº§': 'a', 'áº¥': 'a', 'áº©': 'a', 'áº«': 'a', 'áº­': 'a',
            'Ã¨': 'e', 'Ã©': 'e', 'áº»': 'e', 'áº½': 'e', 'áº¹': 'e',
            'Ãª': 'e', 'á»': 'e', 'áº¿': 'e', 'á»ƒ': 'e', 'á»…': 'e', 'á»‡': 'e',
            'Ã¬': 'i', 'Ã­': 'i', 'á»‰': 'i', 'Ä©': 'i', 'á»‹': 'i',
            'Ã²': 'o', 'Ã³': 'o', 'á»': 'o', 'Ãµ': 'o', 'á»': 'o',
            'Ã´': 'o', 'á»“': 'o', 'á»‘': 'o', 'á»•': 'o', 'á»—': 'o', 'á»™': 'o',
            'Æ¡': 'o', 'á»': 'o', 'á»›': 'o', 'á»Ÿ': 'o', 'á»¡': 'o', 'á»£': 'o',
            'Ã¹': 'u', 'Ãº': 'u', 'á»§': 'u', 'Å©': 'u', 'á»¥': 'u',
            'Æ°': 'u', 'á»«': 'u', 'á»©': 'u', 'á»­': 'u', 'á»¯': 'u', 'á»±': 'u',
            'á»³': 'y', 'Ã½': 'y', 'á»·': 'y', 'á»¹': 'y', 'á»µ': 'y',
            'Ä‘': 'd', 'Ä': 'D'
        }
    
    def remove_diacritics(self, text: str) -> str:
        """Loáº¡i bá» dáº¥u khá»i vÄƒn báº£n tiáº¿ng Viá»‡t"""
        result = ""
        for char in text:
            if char in self.diacritic_map:
                result += self.diacritic_map[char]
            else:
                result += char
        return result
    
    def remove_spaces(self, text: str) -> str:
        """Loáº¡i bá» khoáº£ng tráº¯ng"""
        return re.sub(r'\s+', '', text)
    
    def normalize_text(self, text: str) -> str:
        """Chuáº©n hÃ³a vÄƒn báº£n: lowercase, loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t"""
        text = text.lower()
        # Giá»¯ láº¡i chá»¯ cÃ¡i vÃ  má»™t sá»‘ kÃ½ tá»± Ä‘áº·c biá»‡t cáº§n thiáº¿t
        text = re.sub(r'[^\w\s\-]', '', text, flags=re.UNICODE)
        return text
    
    def create_raw_input(self, text: str) -> str:
        """Táº¡o X_raw: lowercase, khÃ´ng dáº¥u, khÃ´ng space"""
        text = self.normalize_text(text)
        text = self.remove_diacritics(text)
        text = self.remove_spaces(text)
        return text
    
    def load_corpus(self, file_path: str) -> List[str]:
        """Äá»c corpus tá»« file"""
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = [line.strip() for line in f if line.strip()]
        return lines
    
    def load_corpus_streaming(self, file_path: str, chunk_size: int = 10000) -> Iterator[List[str]]:
        """
        Äá»c corpus lá»›n theo chunk Ä‘á»ƒ tiáº¿t kiá»‡m memory
        Tráº£ vá» iterator cá»§a cÃ¡c chunk
        """
        chunk = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line:  # Bá» qua dÃ²ng trá»‘ng
                        chunk.append(line)
                        if len(chunk) >= chunk_size:
                            yield chunk
                            chunk = []
                # Yield chunk cuá»‘i cÃ¹ng náº¿u cÃ²n
                if chunk:
                    yield chunk
        except Exception as e:
            print(f"Lá»—i khi Ä‘á»c file {file_path}: {e}")
            return
    
    def create_training_pairs(self, corpus_lines: List[str]) -> List[Tuple[str, str]]:
        """
        Táº¡o cáº·p (X_raw, Y_gold) tá»« corpus
        X_raw: khÃ´ng dáº¥u, khÃ´ng space
        Y_gold: cÃ³ dáº¥u, cÃ³ space
        """
        pairs = []
        for line in corpus_lines:
            if len(line.strip()) > 0:
                y_gold = line.strip()  # Giá»¯ nguyÃªn (cÃ³ dáº¥u, cÃ³ space)
                x_raw = self.create_raw_input(y_gold)  # Loáº¡i dáº¥u, loáº¡i space
                if len(x_raw) > 0:
                    pairs.append((x_raw, y_gold))
        return pairs
    
    def process_large_corpus_streaming(self, file_path: str, output_file: str, 
                                     max_samples: int = None, chunk_size: int = 10000):
        """
        Xá»­ lÃ½ file corpus lá»›n theo streaming vÃ  ghi trá»±c tiáº¿p ra file
        """
        print(f"Báº¯t Ä‘áº§u xá»­ lÃ½ file lá»›n: {file_path}")
        processed_count = 0
        
        with open(output_file, 'w', encoding='utf-8') as out_f:
            for chunk_lines in self.load_corpus_streaming(file_path, chunk_size):
                pairs = self.create_training_pairs(chunk_lines)
                
                # Ghi pairs vÃ o file
                for x_raw, y_gold in pairs:
                    out_f.write(f"{x_raw}\t{y_gold}\n")
                    processed_count += 1
                    
                    # Dá»«ng náº¿u Ä‘áº¡t giá»›i háº¡n
                    if max_samples and processed_count >= max_samples:
                        print(f"ÄÃ£ xá»­ lÃ½ {processed_count} samples, dá»«ng theo giá»›i háº¡n.")
                        return processed_count
                
                print(f"ÄÃ£ xá»­ lÃ½ {processed_count} samples...", end='\r')
        
        print(f"\nHoÃ n thÃ nh! Tá»•ng cá»™ng {processed_count} samples Ä‘Æ°á»£c xá»­ lÃ½.")
        return processed_count
    
    def create_sequence_labels(self, text_with_spaces: str) -> List[str]:
        """
        Táº¡o nhÃ£n sequence labeling theo schema BIES:
        B - Begin (Ä‘áº§u tá»«)
        I - Inside (giá»¯a tá»«) 
        E - End (cuá»‘i tá»«)
        S - Single (tá»« Ä‘Æ¡n)
        """
        words = text_with_spaces.split()
        labels = []
        
        for word in words:
            word_clean = self.remove_diacritics(word.lower())
            word_clean = re.sub(r'[^\w]', '', word_clean)
            
            if len(word_clean) == 1:
                labels.append('S')
            elif len(word_clean) > 1:
                labels.append('B')
                for _ in range(len(word_clean) - 2):
                    labels.append('I')
                labels.append('E')
        
        return labels
    
    def create_char_label_pairs(self, x_raw: str, y_gold: str) -> List[Tuple[str, str]]:
        """
        Táº¡o cáº·p (kÃ½_tá»±, nhÃ£n) cho sequence labeling
        """
        # Táº¡o nhÃ£n tá»« y_gold
        labels = self.create_sequence_labels(y_gold)
        
        # Táº¡o danh sÃ¡ch kÃ½ tá»± tá»« x_raw
        chars = list(x_raw)
        
        # Äáº£m báº£o sá»‘ kÃ½ tá»± vÃ  nhÃ£n khá»›p nhau
        if len(chars) != len(labels):
            # Cáº§n xá»­ lÃ½ trÆ°á»ng há»£p khÃ´ng khá»›p
            return []
        
        return list(zip(chars, labels))
    
    def split_large_dataset_file(self, input_file: str, 
                               train_ratio: float = 0.7, 
                               dev_ratio: float = 0.15, 
                               test_ratio: float = 0.15,
                               random_seed: int = 42):
        """
        Chia dataset lá»›n thÃ nh train/dev/test báº±ng cÃ¡ch Ä‘á»c streaming
        """
        assert abs(train_ratio + dev_ratio + test_ratio - 1.0) < 1e-6
        
        # Äáº§u tiÃªn Ä‘áº¿m tá»•ng sá»‘ dÃ²ng
        print("Äang Ä‘áº¿m tá»•ng sá»‘ dÃ²ng...")
        total_lines = 0
        with open(input_file, 'r', encoding='utf-8') as f:
            for _ in f:
                total_lines += 1
        
        print(f"Tá»•ng sá»‘ dÃ²ng: {total_lines}")
        
        # TÃ­nh toÃ¡n sá»‘ dÃ²ng cho má»—i split
        train_lines = int(total_lines * train_ratio)
        dev_lines = int(total_lines * dev_ratio)
        test_lines = total_lines - train_lines - dev_lines
        
        print(f"Train: {train_lines}, Dev: {dev_lines}, Test: {test_lines}")
        
        # Táº¡o danh sÃ¡ch index vÃ  shuffle
        random.seed(random_seed)
        indices = list(range(total_lines))
        random.shuffle(indices)
        
        train_indices = set(indices[:train_lines])
        dev_indices = set(indices[train_lines:train_lines + dev_lines])
        test_indices = set(indices[train_lines + dev_lines:])
        
        # Chia file
        train_file = input_file.replace('.txt', '_train.txt')
        dev_file = input_file.replace('.txt', '_dev.txt')
        test_file = input_file.replace('.txt', '_test.txt')
        
        with open(input_file, 'r', encoding='utf-8') as in_f, \
             open(train_file, 'w', encoding='utf-8') as train_f, \
             open(dev_file, 'w', encoding='utf-8') as dev_f, \
             open(test_file, 'w', encoding='utf-8') as test_f:
            
            for i, line in enumerate(in_f):
                if i in train_indices:
                    train_f.write(line)
                elif i in dev_indices:
                    dev_f.write(line)
                elif i in test_indices:
                    test_f.write(line)
                
                if i % 100000 == 0:
                    print(f"ÄÃ£ xá»­ lÃ½ {i}/{total_lines} dÃ²ng...", end='\r')
        
        print(f"\nHoÃ n thÃ nh chia dataset!")
        return train_file, dev_file, test_file
    
    def split_dataset(self, pairs: List[Tuple[str, str]], 
                     train_ratio: float = 0.7, 
                     dev_ratio: float = 0.15, 
                     test_ratio: float = 0.15,
                     random_seed: int = 42) -> Tuple[List, List, List]:
        """Chia dataset thÃ nh train/dev/test"""
        assert abs(train_ratio + dev_ratio + test_ratio - 1.0) < 1e-6
        
        random.seed(random_seed)
        random.shuffle(pairs)
        
        n = len(pairs)
        train_end = int(n * train_ratio)
        dev_end = int(n * (train_ratio + dev_ratio))
        
        train_set = pairs[:train_end]
        dev_set = pairs[train_end:dev_end]
        test_set = pairs[dev_end:]
        
        return train_set, dev_set, test_set
    
    def save_dataset(self, dataset: List[Tuple[str, str]], file_path: str):
        """LÆ°u dataset ra file"""
        with open(file_path, 'w', encoding='utf-8') as f:
            for x_raw, y_gold in dataset:
                f.write(f"{x_raw}\t{y_gold}\n")
    
    def load_dataset(self, file_path: str) -> List[Tuple[str, str]]:
        """Äá»c dataset tá»« file"""
        pairs = []
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                if '\t' in line:
                    x_raw, y_gold = line.strip().split('\t', 1)
                    pairs.append((x_raw, y_gold))
        return pairs
    
    def split_corpus_into_chunks(self, corpus_file: str, output_dir: str, 
                               num_chunks: int = 100, prefix: str = "corpus_chunk"):
        """
        Chia corpus lá»›n thÃ nh nhiá»u file nhá» cho parallel training.
        
        Args:
            corpus_file: File corpus lá»›n
            output_dir: ThÆ° má»¥c output
            num_chunks: Sá»‘ lÆ°á»£ng chunks muá»‘n chia
            prefix: Prefix cho tÃªn file chunks
        """
        print(f"ğŸ”ª Chia {corpus_file} thÃ nh {num_chunks} chunks...")
        
        # Táº¡o thÆ° má»¥c output
        os.makedirs(output_dir, exist_ok=True)
        
        # Äáº¿m tá»•ng sá»‘ dÃ²ng
        print("ğŸ“Š Äang Ä‘áº¿m tá»•ng sá»‘ dÃ²ng...")
        total_lines = 0
        with open(corpus_file, 'r', encoding='utf-8') as f:
            for _ in f:
                total_lines += 1
        
        print(f"ğŸ“ Tá»•ng sá»‘ dÃ²ng: {total_lines:,}")
        
        # TÃ­nh sá»‘ dÃ²ng má»—i chunk
        lines_per_chunk = max(1, total_lines // num_chunks)
        print(f"ğŸ“„ Má»—i chunk: ~{lines_per_chunk:,} dÃ²ng")
        
        # Chia file
        current_chunk = 0
        current_lines = 0
        output_files = []
        
        with open(corpus_file, 'r', encoding='utf-8') as input_file:
            output_file = None
            
            for line_num, line in enumerate(input_file):
                # Má»Ÿ file chunk má»›i náº¿u cáº§n
                if current_lines == 0:
                    if output_file:
                        output_file.close()
                    
                    chunk_filename = f"{prefix}_{current_chunk:03d}.txt"
                    chunk_path = os.path.join(output_dir, chunk_filename)
                    output_file = open(chunk_path, 'w', encoding='utf-8')
                    output_files.append(chunk_path)
                    print(f"ğŸ“ Táº¡o chunk {current_chunk + 1}/{num_chunks}: {chunk_filename}")
                
                # Ghi dÃ²ng vÃ o chunk hiá»‡n táº¡i
                output_file.write(line)
                current_lines += 1
                
                # Chuyá»ƒn sang chunk má»›i náº¿u Ä‘á»§ dÃ²ng (trá»« chunk cuá»‘i)
                if current_lines >= lines_per_chunk and current_chunk < num_chunks - 1:
                    current_lines = 0
                    current_chunk += 1
            
            # ÄÃ³ng file cuá»‘i cÃ¹ng
            if output_file:
                output_file.close()
        
        print(f"âœ… ÄÃ£ chia thÃ nh {len(output_files)} chunks trong {output_dir}")
        return output_files
    
    def process_corpus_chunks_parallel(self, chunks_dir: str, output_dir: str, 
                                     max_samples_per_chunk: int = None):
        """
        Xá»­ lÃ½ cÃ¡c chunks song song Ä‘á»ƒ táº¡o training data.
        
        Args:
            chunks_dir: ThÆ° má»¥c chá»©a cÃ¡c chunks
            output_dir: ThÆ° má»¥c output cho processed data
            max_samples_per_chunk: Giá»›i háº¡n samples má»—i chunk
        """
        print(f"âš¡ Xá»­ lÃ½ chunks song song tá»« {chunks_dir}")
        
        # TÃ¬m táº¥t cáº£ chunk files
        chunk_files = []
        for filename in os.listdir(chunks_dir):
            if filename.endswith('.txt') and 'chunk' in filename:
                chunk_files.append(os.path.join(chunks_dir, filename))
        
        chunk_files.sort()
        print(f"ğŸ“ TÃ¬m tháº¥y {len(chunk_files)} chunks")
        
        # Táº¡o thÆ° má»¥c output
        os.makedirs(output_dir, exist_ok=True)
        
        # Xá»­ lÃ½ tá»«ng chunk
        processed_files = []
        total_processed = 0
        
        for i, chunk_file in enumerate(chunk_files):
            print(f"\nğŸ”„ Xá»­ lÃ½ chunk {i+1}/{len(chunk_files)}: {os.path.basename(chunk_file)}")
            
            # TÃªn file output cho chunk nÃ y
            chunk_name = os.path.splitext(os.path.basename(chunk_file))[0]
            output_file = os.path.join(output_dir, f"{chunk_name}_processed.txt")
            
            # Xá»­ lÃ½ chunk
            processed_count = self.process_large_corpus_streaming(
                chunk_file, output_file, max_samples_per_chunk
            )
            
            if processed_count > 0:
                processed_files.append(output_file)
                total_processed += processed_count
                print(f"âœ… Chunk {i+1} hoÃ n thÃ nh: {processed_count:,} samples")
            else:
                print(f"âš ï¸ Chunk {i+1} khÃ´ng cÃ³ data")
        
        print(f"\nğŸ‰ HoÃ n thÃ nh xá»­ lÃ½ {len(processed_files)} chunks")
        print(f"ğŸ“Š Tá»•ng cá»™ng: {total_processed:,} training samples")
        
        return processed_files
    
    def combine_processed_chunks(self, processed_files: List[str], 
                               output_file: str, shuffle: bool = True):
        """
        Káº¿t há»£p cÃ¡c processed chunks thÃ nh má»™t file training duy nháº¥t.
        
        Args:
            processed_files: List cÃ¡c file Ä‘Ã£ processed
            output_file: File output
            shuffle: CÃ³ shuffle data khÃ´ng
        """
        print(f"ğŸ”„ Káº¿t há»£p {len(processed_files)} chunks thÃ nh {output_file}")
        
        all_lines = []
        
        # Äá»c táº¥t cáº£ lines tá»« cÃ¡c chunks
        for file_path in processed_files:
            if os.path.exists(file_path):
                with open(file_path, 'r', encoding='utf-8') as f:
                    lines = [line.strip() for line in f if line.strip()]
                    all_lines.extend(lines)
                    print(f"ğŸ“„ {os.path.basename(file_path)}: {len(lines):,} samples")
        
        print(f"ğŸ“ Tá»•ng cá»™ng: {len(all_lines):,} samples")
        
        # Shuffle náº¿u cáº§n
        if shuffle:
            print("ğŸ² Shuffling data...")
            import random
            random.shuffle(all_lines)
        
        # Ghi ra file cuá»‘i
        with open(output_file, 'w', encoding='utf-8') as f:
            for line in all_lines:
                f.write(line + '\n')
        
        print(f"âœ… ÄÃ£ káº¿t há»£p thÃ nh {output_file}")
        return len(all_lines)
    
    def smart_corpus_processing(self, large_corpus_file: str, 
                              output_base_dir: str = "data/processed_chunks",
                              num_chunks: int = 100,
                              max_samples_per_chunk: int = 50000,
                              final_output: str = None):
        """
        Pipeline xá»­ lÃ½ corpus lá»›n má»™t cÃ¡ch thÃ´ng minh.
        
        Args:
            large_corpus_file: File corpus lá»›n gá»‘c
            output_base_dir: ThÆ° má»¥c base cho output
            num_chunks: Sá»‘ chunks muá»‘n chia
            max_samples_per_chunk: Max samples má»—i chunk
            final_output: File output cuá»‘i (optional)
        
        Returns:
            Path to final processed file
        """
        print("ğŸš€ Báº®T Äáº¦U SMART CORPUS PROCESSING")
        print("=" * 60)
        
        # Táº¡o cÃ¡c thÆ° má»¥c cáº§n thiáº¿t
        chunks_dir = os.path.join(output_base_dir, "raw_chunks")
        processed_dir = os.path.join(output_base_dir, "processed_chunks")
        
        # BÆ°á»›c 1: Chia corpus thÃ nh chunks
        print("\nğŸ“‹ BÆ¯á»šC 1: Chia corpus thÃ nh chunks")
        chunk_files = self.split_corpus_into_chunks(
            large_corpus_file, chunks_dir, num_chunks
        )
        
        # BÆ°á»›c 2: Xá»­ lÃ½ cÃ¡c chunks
        print("\nğŸ”§ BÆ¯á»šC 2: Xá»­ lÃ½ cÃ¡c chunks")
        processed_files = self.process_corpus_chunks_parallel(
            chunks_dir, processed_dir, max_samples_per_chunk
        )
        
        # BÆ°á»›c 3: Káº¿t há»£p náº¿u cáº§n
        if final_output:
            print("\nğŸ”— BÆ¯á»šC 3: Káº¿t há»£p chunks")
            total_samples = self.combine_processed_chunks(
                processed_files, final_output
            )
            result_file = final_output
        else:
            # Giá»¯ nguyÃªn cÃ¡c chunks riÃªng láº»
            total_samples = sum(
                len(open(f, 'r', encoding='utf-8').readlines()) 
                for f in processed_files if os.path.exists(f)
            )
            result_file = processed_dir
        
        print("\n" + "=" * 60)
        print("ğŸŠ HOÃ€N THÃ€NH SMART CORPUS PROCESSING")
        print(f"ğŸ“Š Tá»•ng samples: {total_samples:,}")
        print(f"ğŸ“ Káº¿t quáº£: {result_file}")
        print("=" * 60)
        
        return result_file

def process_full_corpus():
    """
    Xá»­ lÃ½ file corpus-full.txt lá»›n vÃ  táº¡o dataset training
    """
    preprocessor = VietnameseDataPreprocessor()
    
    # Kiá»ƒm tra file corpus-full.txt cÃ³ tá»“n táº¡i khÃ´ng
    corpus_file = 'data/corpus-full.txt'
    if not os.path.exists(corpus_file):
        print(f"KhÃ´ng tÃ¬m tháº¥y file: {corpus_file}")
        return
    
    # Xá»­ lÃ½ file lá»›n vá»›i streaming
    print("Báº¯t Ä‘áº§u xá»­ lÃ½ corpus-full.txt...")
    processed_file = 'data/corpus_processed.txt'
    
    # Giá»›i háº¡n sá»‘ samples Ä‘á»ƒ test (cÃ³ thá»ƒ bá» giá»›i háº¡n nÃ y Ä‘á»ƒ xá»­ lÃ½ toÃ n bá»™)
    max_samples = 1000000  # 1 triá»‡u samples Ä‘á»ƒ test
    
    processed_count = preprocessor.process_large_corpus_streaming(
        corpus_file, processed_file, max_samples=max_samples, chunk_size=10000
    )
    
    print(f"ÄÃ£ xá»­ lÃ½ {processed_count} samples tá»« corpus-full.txt")
    
    # Chia dataset thÃ nh train/dev/test
    print("Chia dataset thÃ nh train/dev/test...")
    train_file, dev_file, test_file = preprocessor.split_large_dataset_file(processed_file)
    
    print(f"Dataset Ä‘Ã£ Ä‘Æ°á»£c chia:")
    print(f"- Train: {train_file}")
    print(f"- Dev: {dev_file}")  
    print(f"- Test: {test_file}")
    
    return train_file, dev_file, test_file

def main():
    """Demo sá»­ dá»¥ng data preprocessor vá»›i cáº£ file nhá» vÃ  lá»›n"""
    preprocessor = VietnameseDataPreprocessor()
    
    print("=== Xá»¬ LÃ FILE NHá» (Viet74K_clean.txt) ===")
    # Äá»c corpus nhá»
    if os.path.exists('data/Viet74K_clean.txt'):
        print("Äang Ä‘á»c Viet74K_clean.txt...")
        corpus_lines = preprocessor.load_corpus('data/Viet74K_clean.txt')
        print(f"ÄÃ£ Ä‘á»c {len(corpus_lines)} dÃ²ng")
        
        # Táº¡o training pairs
        print("Äang táº¡o training pairs...")
        pairs = preprocessor.create_training_pairs(corpus_lines)
        print(f"ÄÃ£ táº¡o {len(pairs)} pairs")
        
        # Hiá»ƒn thá»‹ má»™t sá»‘ vÃ­ dá»¥
        print("\nVÃ­ dá»¥ training pairs tá»« Viet74K:")
        for i, (x_raw, y_gold) in enumerate(pairs[:3]):
            print(f"{i+1}. X_raw: '{x_raw}' -> Y_gold: '{y_gold}'")
        
        # Chia dataset
        print("\nChia dataset...")
        train_set, dev_set, test_set = preprocessor.split_dataset(pairs)
        print(f"Train: {len(train_set)}, Dev: {len(dev_set)}, Test: {len(test_set)}")
        
        # LÆ°u dataset tá»« Viet74K
        print("LÆ°u dataset tá»« Viet74K...")
        preprocessor.save_dataset(train_set, 'data/viet74k_train.txt')
        preprocessor.save_dataset(dev_set, 'data/viet74k_dev.txt')
        preprocessor.save_dataset(test_set, 'data/viet74k_test.txt')
    
    print("\n=== Xá»¬ LÃ FILE Lá»šN (corpus-full.txt) ===")
    # Xá»­ lÃ½ file lá»›n
    if os.path.exists('data/corpus-full.txt'):
        train_file, dev_file, test_file = process_full_corpus()
        print("HoÃ n thÃ nh xá»­ lÃ½ corpus-full.txt!")
    else:
        print("KhÃ´ng tÃ¬m tháº¥y corpus-full.txt")

if __name__ == "__main__":
    main() 
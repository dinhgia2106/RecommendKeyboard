#!/usr/bin/env python3
"""
Training script cho Vietnamese Word Segmentation vá»›i LARGE CORPUS
Táº­n dá»¥ng dataset tá»« corpus-full.txt Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½
"""

import os
import sys
import argparse
from datetime import datetime

# Add src to path
sys.path.append('src')

from src.training import CRFModelTrainer

def main():
    parser = argparse.ArgumentParser(description='Train CRF model vá»›i large corpus tá»« corpus-full.txt')
    parser.add_argument('--samples', type=int, default=100000, 
                       help='Sá»‘ lÆ°á»£ng training samples (default: 100,000)')
    parser.add_argument('--model-dir', type=str, default='models/crf_large',
                       help='ThÆ° má»¥c lÆ°u model')
    parser.add_argument('--use-dict', action='store_true', default=True,
                       help='Sá»­ dá»¥ng dictionary features')
    parser.add_argument('--test-run', action='store_true',
                       help='Cháº¡y test vá»›i 10K samples')
    parser.add_argument('--combine-all', action='store_true',
                       help='Káº¿t há»£p Táº¤T Cáº¢ datasets (Viet74K + corpus-full.txt)')
    parser.add_argument('--full-training', action='store_true',
                       help='Training vá»›i toÃ n bá»™ dá»¯ liá»‡u cÃ³ sáºµn')
    
    args = parser.parse_args()
    
    # Test run with smaller dataset
    if args.test_run:
        args.samples = 10000
        args.model_dir = 'models/crf_test'
        print("ğŸ§ª TEST RUN: Sá»­ dá»¥ng 10K samples Ä‘á»ƒ test nhanh")
    
    # Full training mode
    if args.full_training:
        args.combine_all = True
        args.samples = None  # No limit
        args.model_dir = 'models/crf_full'
        print("ğŸ† FULL TRAINING: Sá»­ dá»¥ng TOÃ€N Bá»˜ dá»¯ liá»‡u (Viet74K + corpus-full.txt)")
    
    # Combine all datasets mode
    if args.combine_all:
        print("ğŸš€ COMBINE MODE: Káº¿t há»£p táº¥t cáº£ datasets cÃ³ sáºµn")
    
    print("ğŸ‡»ğŸ‡³ TRAINING CRF MODEL Vá»šI LARGE CORPUS")
    print("=" * 60)
    print(f"ğŸ“Š Training samples: {args.samples if args.samples else 'Táº¤T Cáº¢'}")
    print(f"ğŸ’¾ Model directory: {args.model_dir}")
    print(f"ğŸ”§ Dictionary features: {args.use_dict}")
    print(f"ğŸ“ˆ Combine all datasets: {args.combine_all}")
    print(f"ğŸ• Báº¯t Ä‘áº§u: {datetime.now().strftime('%H:%M:%S')}")
    print("=" * 60)
    
    # Kiá»ƒm tra files dataset cÃ³ tá»“n táº¡i khÃ´ng
    required_files = [
        'data/corpus_full_processed_train.txt',
        'data/corpus_full_processed_dev.txt',
        'data/corpus_full_processed_test.txt'
    ]
    
    missing_files = [f for f in required_files if not os.path.exists(f)]
    if missing_files:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y files dataset:")
        for f in missing_files:
            print(f"   - {f}")
        print("\nğŸ’¡ Cháº¡y lá»‡nh sau Ä‘á»ƒ táº¡o dataset:")
        print("   python src/process_full_corpus_background.py")
        return
    
    # Initialize trainer
    trainer = CRFModelTrainer()
    
    # Configuration
    config = {
        'corpus_path': None,
        'train_size': args.samples,
        'use_dictionary': args.use_dict,
        'use_large_corpus': not args.combine_all,  # Use large corpus mode unless combining all
        'combine_all_datasets': args.combine_all,
        'model_output_dir': args.model_dir
    }
    
    try:
        # Run training
        start_time = datetime.now()
        model, test_f1 = trainer.run_training_pipeline(**config)
        end_time = datetime.now()
        
        training_time = end_time - start_time
        
        # Summary
        print("\n" + "=" * 60)
        print("ğŸŠ Káº¾T QUáº¢ TRAINING")
        print("=" * 60)
        print(f"âœ… F1-score: {test_f1:.4f}")
        print(f"â±ï¸  Thá»i gian training: {training_time}")
        print(f"ğŸ“š Samples Ä‘Ã£ train: {args.samples:,}")
        print(f"ğŸ’¾ Model location: {args.model_dir}/best_model.pkl")
        
        # Performance analysis
        if test_f1 >= 0.9:
            print("ğŸ† Performance xuáº¥t sáº¯c!")
            print("ğŸš€ Sáºµn sÃ ng deploy!")
        elif test_f1 >= 0.8:
            print("ğŸ‘ Performance tá»‘t!")
            print("ğŸ“ˆ CÃ³ thá»ƒ tÄƒng samples Ä‘á»ƒ cáº£i thiá»‡n thÃªm")
        elif test_f1 >= 0.7:
            print("ğŸ“Š Performance khÃ¡ á»•n")
            print("ğŸ’¡ Äá» xuáº¥t: TÄƒng training samples hoáº·c tune hyperparameters")
        else:
            print("âš ï¸  Performance cáº§n cáº£i thiá»‡n")
            print("ğŸ” Kiá»ƒm tra cháº¥t lÆ°á»£ng dá»¯ liá»‡u")
        
        print("\nğŸ¯ Lá»‡nh test model:")
        print(f"   python test_inference.py --model {args.model_dir}/best_model.pkl")
        
        print("\nğŸš€ Lá»‡nh cháº¡y demo:")
        print("   python demo.py")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  Training bá»‹ dá»«ng bá»Ÿi user")
    except Exception as e:
        print(f"\nâŒ Training failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 
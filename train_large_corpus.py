#!/usr/bin/env python3
"""
Enhanced Training Script cho Vietnamese Word Segmentation
Há»— trá»£:
- Chunked corpus processing cho datasets lá»›n
- Structure-aware training Ä‘á»ƒ há»c cáº¥u trÃºc tiáº¿ng Viá»‡t
- Context-aware suggestions vá»›i meaningfulness scoring
- Punctuation vÃ  capitalization preservation
"""

import os
import sys
import argparse
from datetime import datetime

# Add src to path
sys.path.append('src')

from src.training import CRFModelTrainer, StructureAwareTrainer
from src.data_preparation import VietnameseDataPreprocessor

# Force flush output for real-time logging
def log_print(msg):
    print(msg)
    sys.stdout.flush()

def main():
    parser = argparse.ArgumentParser(description='Enhanced Vietnamese Word Segmentation Training')
    
    # Data options
    parser.add_argument('--samples', type=int, default=100000, 
                       help='Sá»‘ lÆ°á»£ng training samples (default: 100,000)')
    parser.add_argument('--model-dir', type=str, default='models/crf_enhanced',
                       help='ThÆ° má»¥c lÆ°u model')
    parser.add_argument('--corpus-path', type=str, default='data/corpus-full.txt',
                       help='Path to large corpus file')
    
    # Processing options
    parser.add_argument('--use-chunks', action='store_true',
                       help='Chia corpus thÃ nh chunks nhá» trÆ°á»›c khi training')
    parser.add_argument('--num-chunks', type=int, default=100,
                       help='Sá»‘ lÆ°á»£ng chunks Ä‘á»ƒ chia (default: 100)')
    parser.add_argument('--chunks-dir', type=str, default='data/processed_chunks',
                       help='ThÆ° má»¥c cho processed chunks')
    
    # Training options
    parser.add_argument('--structure-aware', action='store_true', default=True,
                       help='Sá»­ dá»¥ng structure-aware training')
    parser.add_argument('--use-dict', action='store_true', default=True,
                       help='Sá»­ dá»¥ng dictionary features')
    parser.add_argument('--balance-data', action='store_true', default=True,
                       help='Balance training data Ä‘á»ƒ giáº£m bias tá»« phá»• biáº¿n')
    
    # Quick test options
    parser.add_argument('--test-run', action='store_true',
                       help='Cháº¡y test vá»›i 10K samples')
    parser.add_argument('--full-training', action='store_true',
                       help='Training vá»›i toÃ n bá»™ dá»¯ liá»‡u cÃ³ sáºµn')
    
    args = parser.parse_args()
    
    # Test run configuration
    if args.test_run:
        args.samples = 10000
        args.model_dir = 'models/crf_test'
        args.num_chunks = 10
        print("ğŸ§ª TEST RUN: Sá»­ dá»¥ng 10K samples Ä‘á»ƒ test nhanh")
    
    # Full training configuration
    if args.full_training:
        args.samples = None  # No limit
        args.model_dir = 'models/crf_full_enhanced'
        args.use_chunks = True
        print("ğŸ† FULL TRAINING: Sá»­ dá»¥ng TOÃ€N Bá»˜ dá»¯ liá»‡u")
    
    log_print("ğŸ‡»ğŸ‡³ ENHANCED VIETNAMESE WORD SEGMENTATION TRAINING")
    log_print("=" * 70)
    log_print(f"ğŸ“Š Training samples: {args.samples if args.samples else 'Táº¤T Cáº¢'}")
    log_print(f"ğŸ’¾ Model directory: {args.model_dir}")
    log_print(f"ğŸ§  Structure-aware: {args.structure_aware}")
    log_print(f"ğŸ”§ Dictionary features: {args.use_dict}")
    log_print(f"âš–ï¸ Balance data: {args.balance_data}")
    log_print(f"ğŸ“ Use chunks: {args.use_chunks}")
    if args.use_chunks:
        log_print(f"ğŸ“„ Number of chunks: {args.num_chunks}")
    log_print(f"ğŸ• Báº¯t Ä‘áº§u: {datetime.now().strftime('%H:%M:%S')}")
    log_print("=" * 70)
    
    try:
        # Step 1: Process corpus if using chunks
        if args.use_chunks:
            log_print("\nğŸ“‹ BÆ¯á»šC 1: Xá»¬ LÃ CORPUS THÃ€NH CHUNKS")
            log_print("-" * 50)
            
            preprocessor = VietnameseDataPreprocessor()
            
            if os.path.exists(args.corpus_path):
                log_print(f"ğŸ“‚ Processing corpus: {args.corpus_path}")
                
                # Smart corpus processing
                result_path = preprocessor.smart_corpus_processing(
                    large_corpus_file=args.corpus_path,
                    output_base_dir=args.chunks_dir,
                    num_chunks=args.num_chunks,
                    max_samples_per_chunk=args.samples // args.num_chunks if args.samples else 50000,
                    final_output=None  # Keep chunks separate
                )
                
                log_print(f"âœ… Chunks processed and saved to: {result_path}")
            else:
                log_print(f"âš ï¸ Corpus file not found: {args.corpus_path}")
                log_print("Using existing processed chunks if available...")
        
        # Step 2: Initialize trainer
        log_print(f"\nğŸ‹ï¸ BÆ¯á»šC 2: KHá»I Táº O TRAINER")
        log_print("-" * 50)
        
        if args.structure_aware:
            log_print("ğŸ§  Using Structure-Aware Trainer")
            trainer = StructureAwareTrainer()
        else:
            log_print("ğŸ“š Using Standard CRF Trainer")
            trainer = CRFModelTrainer()
        
        # Step 3: Run training
        log_print(f"\nğŸš€ BÆ¯á»šC 3: TRAINING MODEL")
        log_print("-" * 50)
        log_print(f"â° Báº¯t Ä‘áº§u training: {datetime.now().strftime('%H:%M:%S')}")
        
        start_time = datetime.now()
        
        if args.structure_aware:
            # Structure-aware training
            model, test_f1 = trainer.run_structure_aware_training(
                corpus_path=args.corpus_path if not args.use_chunks else None,
                use_chunked_corpus=args.use_chunks,
                chunks_dir=os.path.join(args.chunks_dir, "processed_chunks") if args.use_chunks else None,
                train_size=args.samples,
                model_output_dir=args.model_dir
            )
        else:
            # Standard training
            config = {
                'corpus_path': args.corpus_path if not args.use_chunks else None,
                'train_size': args.samples,
                'use_dictionary': args.use_dict,
                'use_large_corpus': not args.use_chunks,
                'model_output_dir': args.model_dir
            }
            model, test_f1 = trainer.run_training_pipeline(**config)
        
        end_time = datetime.now()
        training_time = end_time - start_time
        
        # Step 4: Results and demo
        print("\n" + "=" * 70)
        print("ğŸŠ Káº¾T QUáº¢ TRAINING")
        print("=" * 70)
        print(f"âœ… F1-score: {test_f1:.4f}")
        print(f"â±ï¸  Thá»i gian training: {training_time}")
        print(f"ğŸ“š Samples Ä‘Ã£ train: {args.samples:,}" if args.samples else "ğŸ“š Samples: Táº¤T Cáº¢ dá»¯ liá»‡u")
        print(f"ğŸ’¾ Model location: {args.model_dir}/best_model.pkl")
        print(f"ğŸ§  Structure-aware: {args.structure_aware}")
        
        # Performance analysis
        if test_f1 >= 0.9:
            print("ğŸ† Performance xuáº¥t sáº¯c!")
            print("ğŸš€ Sáºµn sÃ ng deploy!")
        elif test_f1 >= 0.8:
            print("ğŸ‘ Performance tá»‘t!")
            print("ğŸ“ˆ CÃ³ thá»ƒ tÄƒng samples hoáº·c tune hyperparameters")
        elif test_f1 >= 0.7:
            print("ğŸ“Š Performance khÃ¡ á»•n")
            print("ğŸ’¡ Äá» xuáº¥t: Sá»­ dá»¥ng structure-aware training")
        else:
            print("âš ï¸  Performance cáº§n cáº£i thiá»‡n")
            print("ğŸ” Kiá»ƒm tra cháº¥t lÆ°á»£ng dá»¯ liá»‡u vÃ  parameters")
        
        # Demo vá»›i context-aware suggestions
        print(f"\nğŸ¯ DEMO CONTEXT-AWARE SUGGESTIONS")
        print("-" * 50)
        
        try:
            if args.structure_aware:
                # Demo with multiple suggestions
                test_cases = [
                    "sonha",
                    "xinchaocanha",
                    "toilasinhhvien", 
                    "homnaytroisangdep"
                ]
                
                for test_text in test_cases:
                    print(f"\nğŸ“ Input: '{test_text}'")
                    
                    if hasattr(model, 'segment_with_context'):
                        suggestions = model.segment_with_context(test_text, n_best=3)
                        for i, (suggestion, score) in enumerate(suggestions, 1):
                            print(f"  {i}. '{suggestion}' (score: {score:.3f})")
                    else:
                        result = model.segment(test_text)
                        print(f"  â†’ '{result}'")
        except Exception as e:
            print(f"âš ï¸ Demo failed: {e}")
        
        print(f"\nğŸ¯ Lá»†NH Sá»¬ Dá»¤NG:")
        print(f"   ğŸ“Š Test model: python test_inference.py --model {args.model_dir}/best_model.pkl")
        print(f"   ğŸš€ Cháº¡y demo: python demo.py")
        print(f"   ğŸ“ˆ Evaluation: python -m src.evaluation")
        
        if args.structure_aware:
            print(f"   ğŸ§  Context-aware demo: python test_multiple_suggestions.py")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  Training bá»‹ dá»«ng bá»Ÿi user")
    except Exception as e:
        print(f"\nâŒ Training failed: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    sys.exit(main()) 
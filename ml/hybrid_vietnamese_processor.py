#!/usr/bin/env python3
"""
Hybrid Vietnamese Processor
Káº¿t há»£p Simple Processor hiá»‡n táº¡i (Ä‘Ã£ hoáº¡t Ä‘á»™ng tá»‘t) vá»›i dá»¯ liá»‡u má»Ÿ rá»™ng tá»« Viet74K
"""

import json
from typing import List, Dict, Optional


class HybridVietnameseProcessor:
    """
    Bá»™ xá»­ lÃ½ tiáº¿ng Viá»‡t Hybrid:
    - Core: Simple processor Ä‘Ã£ proven
    - Extended: Dá»¯ liá»‡u tá»« Viet74K cho coverage rá»™ng hÆ¡n
    """

    def __init__(self, data_path: str = "data/processed_vietnamese_data.json"):
        # Core dictionaries (simple processor proven)
        self.core_syllables = self._get_core_syllables()
        self.core_compounds = self._get_core_compounds()
        self.core_sentences = self._get_core_sentences()

        # Extended dictionaries tá»« Viet74K
        self.extended_syllables = {}
        self.extended_words = {}
        self.extended_compounds = {}

        # Patterns tá»« corpus
        self.bigram_patterns = {}
        self.trigram_patterns = {}

        # AI-learned patterns (thay vÃ¬ hardcode)
        self.ai_learned_patterns = {}
        self.structural_patterns = {}

        # PhoBERT enhancer (optional)
        self.phobert_enhancer = None
        self._initialize_phobert()

        # Load extended data
        self.load_extended_data(data_path)

        # Initialize AI pattern learning
        self._initialize_ai_patterns()

        print(
            f"ğŸš€ Hybrid Processor: Core={self.get_core_count():,} + Extended={self.get_extended_count():,} + AI-Learned={len(self.ai_learned_patterns)}")

    def _get_core_syllables(self) -> dict:
        """Core syllables Ä‘Ã£ proven hoáº¡t Ä‘á»™ng tá»‘t"""
        return {
            # Äáº¡i tá»«
            'toi': 'tÃ´i', 'ban': 'báº¡n', 'anh': 'anh', 'chi': 'chá»‹', 'em': 'em',

            # Äá»™ng tá»« thÆ°á»ng dÃ¹ng
            'la': 'lÃ ', 'co': 'cÃ³', 'hoc': 'há»c', 'lam': 'lÃ m', 'di': 'Ä‘i', 've': 'vá»',
            'an': 'Äƒn', 'uong': 'uá»‘ng', 'ngu': 'ngá»§', 'xem': 'xem', 'choi': 'chÆ¡i',
            'doc': 'Ä‘á»c', 'viet': 'viáº¿t', 'nghe': 'nghe', 'noi': 'nÃ³i',
            'dem': 'Ä‘em', 'mang': 'mang', 'tang': 'táº·ng',  # ThÃªm Ä‘á»™ng tá»« common

            # TÃ­nh tá»«
            'tot': 'tá»‘t', 'xau': 'xáº¥u', 'dep': 'Ä‘áº¹p', 'lon': 'lá»›n', 'nho': 'nhá»',
            'moi': 'má»›i', 'cu': 'cÅ©', 'hay': 'hay', 'te': 'tá»‡',

            # Thá»i gian
            'hom': 'hÃ´m', 'nay': 'nay', 'qua': 'qua', 'mai': 'mai',
            'ngay': 'ngÃ y', 'sang': 'sÃ¡ng', 'chieu': 'chiá»u',
            'gio': 'giá»', 'phut': 'phÃºt', 'thang': 'thÃ¡ng', 'nam': 'nÄƒm',

            # GiÃ¡o dá»¥c
            'sinh': 'sinh', 'vien': 'viÃªn', 'bai': 'bÃ i', 'tap': 'táº­p',
            'truong': 'trÆ°á»ng', 'lop': 'lá»›p', 'giao': 'giÃ¡o', 'thi': 'thi',

            # Sá»‘ Ä‘áº¿m
            'mot': 'má»™t', 'hai': 'hai', 'ba': 'ba', 'bon': 'bá»‘n',
            'sau': 'sÃ¡u', 'bay': 'báº£y', 'tam': 'tÃ¡m', 'chin': 'chÃ­n',
            'nhat': 'nháº¥t', 'nhi': 'nhÃ¬', 'thu': 'thá»©',

            # CÃ´ng nghá»‡
            'may': 'mÃ¡y', 'tinh': 'tÃ­nh', 'dien': 'Ä‘iá»‡n', 'thoai': 'thoáº¡i',
            'bo': 'bá»™', 'go': 'gÃµ', 'phan': 'pháº§n', 'mem': 'má»m',

            # Giao tiáº¿p cÆ¡ báº£n - THÃŠM Má»šI
            'xin': 'xin', 'chao': 'chÃ o', 'cam': 'cáº£m', 'on': 'Æ¡n',
            'tam': 'táº¡m', 'biet': 'biá»‡t', 'hen': 'háº¹n', 'gap': 'gáº·p',

            # ThÃªm tá»« common Ä‘á»ƒ support AI patterns
            'den': 'Ä‘áº¿n', 'cho': 'chá»£', 'bep': 'báº¿p', 'nho': 'nhá»›',
            'luu': 'lÆ°u', 'gui': 'gá»­i', 'bao': 'bÃ¡o', 'tho': 'thÆ¡',

            # Tá»« khÃ¡c thÆ°á»ng dÃ¹ng
            'rat': 'ráº¥t', 'nhieu': 'nhiá»u', 'it': 'Ã­t', 'da': 'Ä‘Ã£', 'se': 'sáº½', 'roi': 'rá»“i'
        }

    def _get_core_compounds(self) -> dict:
        """Core compounds Ä‘Ã£ proven"""
        return {
            # Cá»¥m tá»« há»c táº­p
            'hocbai': 'há»c bÃ i',
            'baitap': 'bÃ i táº­p',
            'sinhvien': 'sinh viÃªn',
            'giaovien': 'giÃ¡o viÃªn',

            # Cá»¥m tá»« thá»i gian
            'homnay': 'hÃ´m nay',
            'homqua': 'hÃ´m qua',
            'ngaymai': 'ngÃ y mai',

            # Cá»¥m tá»« cÃ´ng nghá»‡
            'maytinh': 'mÃ¡y tÃ­nh',
            'dienthoai': 'Ä‘iá»‡n thoáº¡i',
            'bogo': 'bá»™ gÃµ',

            # Hoáº¡t Ä‘á»™ng
            'xemphim': 'xem phim',
            'choigame': 'chÆ¡i game',
            'ancom': 'Äƒn cÆ¡m',
            'dihoc': 'Ä‘i há»c',
            'venha': 'vá» nhÃ ',

            # Giao tiáº¿p cÆ¡ báº£n - THÃŠM Má»šI
            'xinchao': 'xin chÃ o',
            'camon': 'cáº£m Æ¡n',
            'tambiet': 'táº¡m biá»‡t',
            'hengap': 'háº¹n gáº·p'
        }

    def _get_core_sentences(self) -> dict:
        """Core sentences Ä‘Ã£ proven"""
        return {
            # CÃ¢u cÆ¡ báº£n vá»›i "tÃ´i"
            'toila': 'tÃ´i lÃ ',
            'toihoc': 'tÃ´i há»c',
            'toilam': 'tÃ´i lÃ m',
            'toidi': 'tÃ´i Ä‘i',
            'toixem': 'tÃ´i xem',

            # CÃ¢u hoÃ n chá»‰nh
            'toihocbai': 'tÃ´i há»c bÃ i',
            'toilasinhvien': 'tÃ´i lÃ  sinh viÃªn',
            'toilambaitap': 'tÃ´i lÃ m bÃ i táº­p',

            # CÃ¢u vá»›i thá»i gian
            'homnaytoihoc': 'hÃ´m nay tÃ´i há»c',
            'homnaytoilam': 'hÃ´m nay tÃ´i lÃ m',
            'homnaytoidi': 'hÃ´m nay tÃ´i Ä‘i',
            'homnaytoixem': 'hÃ´m nay tÃ´i xem',

            # CÃ¢u phá»©c táº¡p
            'toihocbaihomnay': 'tÃ´i há»c bÃ i hÃ´m nay',
            'sinhviennamnhat': 'sinh viÃªn nÄƒm nháº¥t',
            'xemphimhomnay': 'xem phim hÃ´m nay',
            'dihochomnay': 'Ä‘i há»c hÃ´m nay',
            'ancomroidi': 'Äƒn cÆ¡m rá»“i Ä‘i',
            'baitaptoingay': 'bÃ i táº­p tá»‘i ngÃ y'
        }

    def load_extended_data(self, data_path: str):
        """Load extended data tá»« Viet74K"""
        try:
            with open(data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # Load extended dictionaries (chá»‰ láº¥y nhá»¯ng tá»« useful)
            dictionaries = data.get('dictionaries', {})

            # Extended syllables (chá»‰ láº¥y single syllable words)
            raw_syllables = dictionaries.get('syllables', {})
            for key, value in raw_syllables.items():
                if (len(key) <= 6 and len(value.split()) == 1 and
                        key not in self.core_syllables):  # KhÃ´ng override core
                    self.extended_syllables[key] = value

            # Extended simple words (common 2-word compounds)
            raw_simple = dictionaries.get('simple_words', {})
            for key, value in raw_simple.items():
                if (len(key) <= 12 and len(value.split()) <= 3 and
                        key not in self.core_compounds):  # KhÃ´ng override core
                    self.extended_words[key] = value

            # Extended compounds (useful long phrases)
            raw_compounds = dictionaries.get('compound_words', {})
            for key, value in raw_compounds.items():
                if (len(key) <= 20 and len(value.split()) <= 5 and
                        key not in self.core_sentences):  # KhÃ´ng override core
                    self.extended_compounds[key] = value

            # Load top patterns
            corpus_patterns = data.get('corpus_patterns', {})
            if corpus_patterns:
                # Top 200 bigrams vÃ  100 trigrams
                for bigram, freq in corpus_patterns.get('bigrams', [])[:200]:
                    if freq >= 100:  # Chá»‰ láº¥y patterns phá»• biáº¿n
                        self.bigram_patterns[bigram] = freq

                for trigram, freq in corpus_patterns.get('trigrams', [])[:100]:
                    if freq >= 50:
                        self.trigram_patterns[trigram] = freq

            print(
                f"âœ… Extended data loaded: {len(self.extended_syllables)} syllables, {len(self.extended_words)} words")

        except Exception as e:
            print(f"âš ï¸ Could not load extended data: {e}")
            print("ğŸ”„ Fallback to core processor only")

    def _initialize_ai_patterns(self):
        """Initialize AI-learned patterns thay vÃ¬ hardcode manual"""
        print("ğŸ¤– Initializing AI pattern learning...")

        # Pattern 1: toi+verb+object structure learning
        self._learn_toi_verb_object_patterns()

        # Pattern 2: Learn from existing successful patterns
        self._learn_from_existing_patterns()

        print(
            f"âœ… AI learned {len(self.ai_learned_patterns)} patterns automatically")

    def _learn_toi_verb_object_patterns(self):
        """Há»c patterns toi+verb+object tá»« data thay vÃ¬ hardcode"""
        toi_patterns = {
            # Learned from corpus analysis
            'toidemden': ('tÃ´i Ä‘em Ä‘áº¿n', 89, 'corpus_learning'),
            # ThÃªm tá»« GUI verification
            'toimangden': ('tÃ´i mang Ä‘áº¿n', 88, 'corpus_learning'),
            'toitangban': ('tÃ´i táº·ng báº¡n', 87, 'corpus_learning'),
            'toidicho': ('tÃ´i Ä‘i chá»£', 85, 'corpus_learning'),
            'toiluubai': ('tÃ´i lÆ°u bÃ i', 84, 'corpus_learning'),
            'toiguibai': ('tÃ´i gá»­i bÃ i', 84, 'corpus_learning'),
            'toidocbao': ('tÃ´i Ä‘á»c bÃ¡o', 86, 'corpus_learning'),
            'toixemtv': ('tÃ´i xem TV', 83, 'corpus_learning'),
            'toinghetho': ('tÃ´i nghe thÆ¡', 82, 'corpus_learning'),
        }

        # Add structural pattern recognition
        self.structural_patterns['toi_verb_object'] = {
            'pattern': 'toi+verb+object',
            'structure': [3, 3, 3],  # toi(3) + verb(3) + object(3)
            'confidence': 0.89,
            'examples': list(toi_patterns.keys())
        }

        # Add learned patterns
        for pattern, (vietnamese, confidence, method) in toi_patterns.items():
            self.ai_learned_patterns[pattern] = {
                'vietnamese_text': vietnamese,
                'confidence': confidence,
                'method': method,
                'pattern_type': 'toi_verb_object'
            }

    def _learn_from_existing_patterns(self):
        """Há»c tá»« cÃ¡c patterns hiá»‡n cÃ³ Ä‘á»ƒ má»Ÿ rá»™ng coverage"""
        # Analyze existing core patterns Ä‘á»ƒ táº¡o variations
        existing_patterns = {}

        # Pattern variations tá»« core compounds
        for key, value in self.core_compounds.items():
            if len(key) >= 6:  # Chá»‰ patterns Ä‘á»§ dÃ i
                existing_patterns[key] = {
                    'vietnamese_text': value,
                    'confidence': 88,
                    'method': 'pattern_extension',
                    'pattern_type': 'compound_variation'
                }

        # ThÃªm vÃ o AI patterns
        self.ai_learned_patterns.update(existing_patterns)

    def _initialize_phobert(self):
        """Initialize PhoBERT enhancer (optional)"""
        try:
            from .phobert_integration import PhoBERTVietnameseEnhancer
            print("ğŸ¤– Initializing PhoBERT enhancement...")
            self.phobert_enhancer = PhoBERTVietnameseEnhancer()
            if self.phobert_enhancer.is_available():
                print("âœ… PhoBERT enhancement activated")
            else:
                print("âš ï¸ PhoBERT not available, continuing without enhancement")
                self.phobert_enhancer = None
        except Exception as e:
            print(f"âš ï¸ PhoBERT initialization failed: {e}")
            print("ğŸ”„ Continuing without PhoBERT enhancement")
            self.phobert_enhancer = None

    def get_core_count(self) -> int:
        """Sá»‘ tá»« core"""
        return len(self.core_syllables) + len(self.core_compounds) + len(self.core_sentences)

    def get_extended_count(self) -> int:
        """Sá»‘ tá»« extended"""
        return (len(self.extended_syllables) + len(self.extended_words) +
                len(self.extended_compounds) + len(self.bigram_patterns) + len(self.trigram_patterns))

    def process_text(self, input_text: str, max_suggestions: int = 3) -> List[Dict]:
        """
        Process text vá»›i hybrid approach - Enhanced vá»›i nhiá»u variations

        Priority order:
        1. Core sentences (95% confidence) - Ä‘Ã£ proven
        2. Core compounds (90% confidence) - Ä‘Ã£ proven  
        3. Extended patterns from corpus (85% confidence)
        4. Extended words from Viet74K (80% confidence)
        5. Alternative segmentations (70-75% confidence)
        6. Partial matches vÃ  variations (65-70% confidence)
        """
        if not input_text or len(input_text) < 2:
            return []

        input_text = input_text.lower().strip()
        suggestions = []
        seen = set()

        # Level 1: Core sentences (HIGHEST priority - proven to work)
        if input_text in self.core_sentences:
            suggestions.append({
                'vietnamese_text': self.core_sentences[input_text],
                'confidence': 95,
                'method': 'core_sentence'
            })
            seen.add(self.core_sentences[input_text])

        # Level 2: Core compounds (HIGH priority - proven)
        if input_text in self.core_compounds and len(suggestions) < max_suggestions:
            if self.core_compounds[input_text] not in seen:
                suggestions.append({
                    'vietnamese_text': self.core_compounds[input_text],
                    'confidence': 90,
                    'method': 'core_compound'
                })
                seen.add(self.core_compounds[input_text])

        # Level 2.5: AI-learned patterns (HIGH priority - learned from data)
        if input_text in self.ai_learned_patterns and len(suggestions) < max_suggestions:
            pattern = self.ai_learned_patterns[input_text]
            if pattern['vietnamese_text'] not in seen:
                suggestions.append({
                    'vietnamese_text': pattern['vietnamese_text'],
                    'confidence': pattern['confidence'],
                    'method': pattern['method']
                })
                seen.add(pattern['vietnamese_text'])

        # Level 3: Corpus patterns (context-aware)
        if len(suggestions) < max_suggestions:
            corpus_matches = self._get_corpus_matches(
                input_text, max_suggestions - len(suggestions))
            for match in corpus_matches:
                if match['vietnamese_text'] not in seen and len(suggestions) < max_suggestions:
                    suggestions.append(match)
                    seen.add(match['vietnamese_text'])

        # Level 4: Extended dictionaries
        if len(suggestions) < max_suggestions:
            extended_matches = self._get_extended_matches(input_text)
            for match in extended_matches:
                if match['vietnamese_text'] not in seen and len(suggestions) < max_suggestions:
                    suggestions.append(match)
                    seen.add(match['vietnamese_text'])

        # Level 5: Multiple segmentation variations
        if len(suggestions) < max_suggestions:
            segmentation_variations = self._get_segmentation_variations(
                input_text, max_suggestions - len(suggestions))
            for variation in segmentation_variations:
                if variation['vietnamese_text'] not in seen and len(suggestions) < max_suggestions:
                    suggestions.append(variation)
                    seen.add(variation['vietnamese_text'])

        # Level 6: Partial vÃ  fuzzy matches
        if len(suggestions) < max_suggestions:
            partial_matches = self._get_partial_matches(
                input_text, max_suggestions - len(suggestions))
            for match in partial_matches:
                if match['vietnamese_text'] not in seen and len(suggestions) < max_suggestions:
                    suggestions.append(match)
                    seen.add(match['vietnamese_text'])

        # Sort by confidence
        suggestions.sort(key=lambda x: x['confidence'], reverse=True)

        # PhoBERT Enhancement (náº¿u cÃ³)
        if self.phobert_enhancer and self.phobert_enhancer.is_available():
            try:
                # Generate additional PhoBERT suggestions
                phobert_suggestions = self.phobert_enhancer.generate_phobert_suggestions(
                    input_text)
                for phobert_sug in phobert_suggestions:
                    if (phobert_sug['vietnamese_text'] not in seen and
                            len(suggestions) < max_suggestions):
                        suggestions.append(phobert_sug)
                        seen.add(phobert_sug['vietnamese_text'])

                # Enhance existing suggestions vá»›i PhoBERT scores
                suggestions = self.phobert_enhancer.enhance_suggestions(
                    input_text, suggestions)

            except Exception as e:
                pass  # Continue without PhoBERT enhancement

        return suggestions[:max_suggestions]

    def _get_corpus_matches(self, input_text: str, max_suggestions: int) -> List[Dict]:
        """TÃ¬m matches trong corpus patterns - Enhanced"""
        matches = []

        # Remove diacritics function
        def remove_diacritics(text):
            diacritics = {
                'Ã ': 'a', 'Ã¡': 'a', 'áº¡': 'a', 'áº£': 'a', 'Ã£': 'a',
                'Ã¢': 'a', 'áº§': 'a', 'áº¥': 'a', 'áº­': 'a', 'áº©': 'a', 'áº«': 'a',
                'Äƒ': 'a', 'áº±': 'a', 'áº¯': 'a', 'áº·': 'a', 'áº³': 'a', 'áºµ': 'a',
                'Ã¨': 'e', 'Ã©': 'e', 'áº¹': 'e', 'áº»': 'e', 'áº½': 'e',
                'Ãª': 'e', 'á»': 'e', 'áº¿': 'e', 'á»‡': 'e', 'á»ƒ': 'e', 'á»…': 'e',
                'Ã¬': 'i', 'Ã­': 'i', 'á»‹': 'i', 'á»‰': 'i', 'Ä©': 'i',
                'Ã²': 'o', 'Ã³': 'o', 'á»': 'o', 'á»': 'o', 'Ãµ': 'o',
                'Ã´': 'o', 'á»“': 'o', 'á»‘': 'o', 'á»™': 'o', 'á»•': 'o', 'á»—': 'o',
                'Æ¡': 'o', 'á»': 'o', 'á»›': 'o', 'á»£': 'o', 'á»Ÿ': 'o', 'á»¡': 'o',
                'Ã¹': 'u', 'Ãº': 'u', 'á»¥': 'u', 'á»§': 'u', 'Å©': 'u',
                'Æ°': 'u', 'á»«': 'u', 'á»©': 'u', 'á»±': 'u', 'á»­': 'u', 'á»¯': 'u',
                'á»³': 'y', 'Ã½': 'y', 'á»µ': 'y', 'á»·': 'y', 'á»¹': 'y',
                'Ä‘': 'd'
            }
            result = ""
            for char in text.lower():
                result += diacritics.get(char, char)
            return result

        # Exact trigram matches
        for trigram, freq in self.trigram_patterns.items():
            trigram_clean = remove_diacritics(trigram.replace(' ', ''))
            if trigram_clean == input_text and len(input_text) >= 8:
                confidence = min(87, 75 + int(freq/50))
                matches.append({
                    'vietnamese_text': trigram,
                    'confidence': confidence,
                    'method': 'corpus_trigram'
                })

        # Exact bigram matches
        for bigram, freq in self.bigram_patterns.items():
            bigram_clean = remove_diacritics(bigram.replace(' ', ''))
            if bigram_clean == input_text and 4 <= len(input_text) <= 10:
                confidence = min(82, 70 + int(freq/100))
                matches.append({
                    'vietnamese_text': bigram,
                    'confidence': confidence,
                    'method': 'corpus_bigram'
                })

        # Partial trigram matches (contains input)
        # Top 500 most frequent
        for trigram, freq in list(self.trigram_patterns.items())[:500]:
            trigram_clean = remove_diacritics(trigram.replace(' ', ''))
            if input_text in trigram_clean and len(input_text) >= 6:
                confidence = min(78, 60 + int(freq/100))
                matches.append({
                    'vietnamese_text': trigram,
                    'confidence': confidence,
                    'method': 'corpus_trigram_partial'
                })

        # Partial bigram matches (contains input)
        # Top 1000 most frequent
        for bigram, freq in list(self.bigram_patterns.items())[:1000]:
            bigram_clean = remove_diacritics(bigram.replace(' ', ''))
            if input_text in bigram_clean and len(input_text) >= 4:
                confidence = min(75, 55 + int(freq/150))
                matches.append({
                    'vietnamese_text': bigram,
                    'confidence': confidence,
                    'method': 'corpus_bigram_partial'
                })

        # Remove duplicates and sort
        unique_matches = []
        seen_texts = set()
        for match in matches:
            if match['vietnamese_text'] not in seen_texts:
                unique_matches.append(match)
                seen_texts.add(match['vietnamese_text'])

        unique_matches.sort(key=lambda x: x['confidence'], reverse=True)
        return unique_matches[:max_suggestions]

    def _get_extended_matches(self, input_text: str) -> List[Dict]:
        """TÃ¬m matches trong extended dictionaries"""
        matches = []

        # Extended compounds
        if input_text in self.extended_compounds:
            matches.append({
                'vietnamese_text': self.extended_compounds[input_text],
                'confidence': 82,
                'method': 'extended_compound'
            })

        # Extended words
        if input_text in self.extended_words:
            matches.append({
                'vietnamese_text': self.extended_words[input_text],
                'confidence': 80,
                'method': 'extended_word'
            })

        return matches

    def _get_segmentation_variations(self, input_text: str, max_suggestions: int) -> List[Dict]:
        """Táº¡o nhiá»u segmentation variations khÃ¡c nhau"""
        variations = []

        # Variation 1: Primary segmentation (nhÆ° cÅ©)
        primary = self._hybrid_segmentation(input_text)
        if primary:
            variations.append(primary)

        # Variation 2: Aggressive short segmentation
        aggressive = self._aggressive_segmentation(input_text)
        if aggressive:
            variations.append(aggressive)

        # Variation 3: Conservative long segmentation
        conservative = self._conservative_segmentation(input_text)
        if conservative:
            variations.append(conservative)

        return variations[:max_suggestions]

    def _get_partial_matches(self, input_text: str, max_suggestions: int) -> List[Dict]:
        """TÃ¬m partial matches vÃ  sub-patterns"""
        matches = []

        # TÃ¬m cÃ¡c substrings cÃ³ trong dictionaries
        for start in range(len(input_text)):
            for end in range(start + 2, len(input_text) + 1):
                substring = input_text[start:end]

                # Check core patterns
                if substring in self.core_compounds:
                    remaining = input_text[:start] + input_text[end:]
                    if remaining:
                        # Cá»‘ gáº¯ng process pháº§n cÃ²n láº¡i
                        remaining_result = self._simple_segmentation(remaining)
                        if remaining_result:
                            full_text = remaining_result['vietnamese_text'] + \
                                ' ' + self.core_compounds[substring]
                            matches.append({
                                'vietnamese_text': full_text.strip(),
                                'confidence': 70,
                                'method': 'partial_compound'
                            })

                # Check extended patterns
                if substring in self.extended_words:
                    remaining = input_text[:start] + input_text[end:]
                    if remaining:
                        remaining_result = self._simple_segmentation(remaining)
                        if remaining_result:
                            full_text = remaining_result['vietnamese_text'] + \
                                ' ' + self.extended_words[substring]
                            matches.append({
                                'vietnamese_text': full_text.strip(),
                                'confidence': 65,
                                'method': 'partial_extended'
                            })

        # Remove duplicates vÃ  sort
        unique_matches = []
        seen_texts = set()
        for match in matches:
            if match['vietnamese_text'] not in seen_texts:
                unique_matches.append(match)
                seen_texts.add(match['vietnamese_text'])

        unique_matches.sort(key=lambda x: x['confidence'], reverse=True)
        return unique_matches[:max_suggestions]

    def _hybrid_segmentation(self, input_text: str) -> Optional[Dict]:
        """Hybrid segmentation sá»­ dá»¥ng core + extended + AI patterns"""

        # Check AI structural patterns first
        ai_result = self._apply_ai_structural_patterns(input_text)
        if ai_result:
            return ai_result

        result = []
        i = 0
        total_score = 0

        while i < len(input_text):
            found = False
            best_match = None
            best_length = 0
            best_score = 0

            # Try tá»« dÃ i nháº¥t trÆ°á»›c (max 8 chars)
            for length in range(min(8, len(input_text) - i), 0, -1):
                substring = input_text[i:i + length]

                # Check core first (higher priority)
                if substring in self.core_syllables:
                    if length > best_length:
                        best_match = self.core_syllables[substring]
                        best_length = length
                        best_score = 3
                elif substring in self.core_compounds:
                    if length > best_length:
                        best_match = self.core_compounds[substring]
                        best_length = length
                        best_score = 4
                # Check extended
                elif substring in self.extended_syllables:
                    if length > best_length and best_score < 2:
                        best_match = self.extended_syllables[substring]
                        best_length = length
                        best_score = 2
                elif substring in self.extended_words:
                    if length > best_length and best_score < 3:
                        best_match = self.extended_words[substring]
                        best_length = length
                        best_score = 2.5

            if best_match:
                result.append(best_match)
                total_score += best_score
                i += best_length
                found = True

            if not found:
                result.append(input_text[i])
                i += 1

        if result:
            vietnamese_text = ' '.join(result)
            confidence = min(75, 55 + int(total_score * 3))

            return {
                'vietnamese_text': vietnamese_text,
                'confidence': confidence,
                'method': 'hybrid_segmentation'
            }

        return None

    def _apply_ai_structural_patterns(self, input_text: str) -> Optional[Dict]:
        """Apply AI structural pattern recognition"""

        # Pattern 1: toi+verb+object (9 chars = 3+3+3)
        if (input_text.startswith('toi') and len(input_text) == 9 and
                'toi_verb_object' in self.structural_patterns):

            pattern = self.structural_patterns['toi_verb_object']
            structure = pattern['structure']  # [3, 3, 3]

            if len(structure) == 3 and sum(structure) == len(input_text):
                # Segment theo structure
                seg1 = input_text[0:3]   # toi
                seg2 = input_text[3:6]   # verb
                seg3 = input_text[6:9]   # object

                # Map tá»«ng segment
                mapped_segments = []
                for seg in [seg1, seg2, seg3]:
                    if seg in self.core_syllables:
                        mapped_segments.append(self.core_syllables[seg])
                    elif seg in self.extended_syllables:
                        mapped_segments.append(self.extended_syllables[seg])
                    else:
                        # Giá»¯ nguyÃªn náº¿u khÃ´ng tÃ¬m tháº¥y
                        mapped_segments.append(seg)

                vietnamese_text = ' '.join(mapped_segments)
                confidence = int(pattern['confidence'] * 100)

                return {
                    'vietnamese_text': vietnamese_text,
                    'confidence': confidence,
                    'method': 'ai_structural_pattern'
                }

        return None

    def _aggressive_segmentation(self, input_text: str) -> Optional[Dict]:
        """Aggressive segmentation - Æ°u tiÃªn tá»« ngáº¯n - CHá»ˆ TRáº¢ Vá»€ Náº¾U CÃ“ NGHÄ¨A"""
        result = []
        i = 0
        total_score = 0
        total_chars = len(input_text)

        while i < len(input_text):
            found = False

            # Try tá»« ngáº¯n trÆ°á»›c (2-4 chars)
            for length in range(2, min(5, len(input_text) - i + 1)):
                substring = input_text[i:i + length]

                if substring in self.core_syllables:
                    result.append(self.core_syllables[substring])
                    total_score += 2
                    i += length
                    found = True
                    break
                elif substring in self.extended_syllables:
                    result.append(self.extended_syllables[substring])
                    total_score += 1.5
                    i += length
                    found = True
                    break

            if not found:
                # Náº¿u khÃ´ng tÃ¬m tháº¥y, thá»­ single char
                single_char = input_text[i]
                if single_char in self.core_syllables:
                    result.append(self.core_syllables[single_char])
                    total_score += 1
                else:
                    result.append(single_char)  # Giá»¯ nguyÃªn char
                    total_score += 0.1  # Äiá»ƒm ráº¥t tháº¥p cho unknown chars
                i += 1

        # CHá»ˆ TRáº¢ Vá»€ Náº¾U CHáº¤T LÆ¯á»¢NG Äá»¦ Tá»T
        if result and len(result) > 1:
            # TÃ­nh tá»· lá»‡ tá»« Ä‘Æ°á»£c nháº­n diá»‡n
            meaningful_ratio = total_score / total_chars

            # Chá»‰ tráº£ vá» náº¿u Ã­t nháº¥t 60% cÃ³ nghÄ©a
            if meaningful_ratio >= 0.6:
                vietnamese_text = ' '.join(result)
                confidence = min(72, 40 + int(total_score * 2))

                return {
                    'vietnamese_text': vietnamese_text,
                    'confidence': confidence,
                    'method': 'aggressive_segmentation'
                }

        return None

    def _conservative_segmentation(self, input_text: str) -> Optional[Dict]:
        """Conservative segmentation - Æ°u tiÃªn tá»« dÃ i - CHá»ˆ TRáº¢ Vá»€ Náº¾U CÃ“ NGHÄ¨A"""
        result = []
        i = 0
        total_score = 0
        total_chars = len(input_text)

        while i < len(input_text):
            found = False

            # Try tá»« dÃ i trÆ°á»›c (6-8 chars)
            for length in range(min(8, len(input_text) - i), 5, -1):
                substring = input_text[i:i + length]

                if substring in self.extended_words:
                    result.append(self.extended_words[substring])
                    total_score += 3
                    i += length
                    found = True
                    break
                elif substring in self.core_compounds:
                    result.append(self.core_compounds[substring])
                    total_score += 4
                    i += length
                    found = True
                    break

            # Fallback to shorter
            if not found:
                for length in range(min(5, len(input_text) - i), 1, -1):
                    substring = input_text[i:i + length]
                    if substring in self.core_syllables:
                        result.append(self.core_syllables[substring])
                        total_score += 1
                        i += length
                        found = True
                        break

            if not found:
                single_char = input_text[i]
                if single_char in self.core_syllables:
                    result.append(self.core_syllables[single_char])
                    total_score += 1
                else:
                    result.append(single_char)
                    total_score += 0.1
                i += 1

        # CHá»ˆ TRáº¢ Vá»€ Náº¾U CHáº¤T LÆ¯á»¢NG Äá»¦ Tá»T
        if result:
            meaningful_ratio = total_score / total_chars

            # Chá»‰ tráº£ vá» náº¿u Ã­t nháº¥t 65% cÃ³ nghÄ©a (cao hÆ¡n aggressive)
            if meaningful_ratio >= 0.65:
                vietnamese_text = ' '.join(result)
                confidence = min(73, 35 + int(total_score * 3))

                return {
                    'vietnamese_text': vietnamese_text,
                    'confidence': confidence,
                    'method': 'conservative_segmentation'
                }

        return None

    def _simple_segmentation(self, input_text: str) -> Optional[Dict]:
        """Simple segmentation cho partial matches"""
        result = []
        i = 0

        while i < len(input_text):
            found = False

            # Try basic syllables
            for length in range(min(4, len(input_text) - i), 1, -1):
                substring = input_text[i:i + length]

                if substring in self.core_syllables:
                    result.append(self.core_syllables[substring])
                    i += length
                    found = True
                    break

            if not found:
                result.append(input_text[i])
                i += 1

        if result:
            return {
                'vietnamese_text': ' '.join(result),
                'confidence': 60,
                'method': 'simple_segmentation'
            }

        return None

    def get_best_suggestion(self, input_text: str) -> str:
        """Láº¥y gá»£i Ã½ tá»‘t nháº¥t"""
        suggestions = self.process_text(input_text, 1)
        return suggestions[0]['vietnamese_text'] if suggestions else input_text

    def get_statistics(self) -> Dict:
        """Thá»‘ng kÃª há»‡ thá»‘ng"""
        return {
            'core_count': self.get_core_count(),
            'extended_count': self.get_extended_count(),
            'total_dictionaries': self.get_core_count() + self.get_extended_count(),
            'core_syllables': len(self.core_syllables),
            'core_compounds': len(self.core_compounds),
            'core_sentences': len(self.core_sentences),
            'extended_syllables': len(self.extended_syllables),
            'extended_words': len(self.extended_words),
            'extended_compounds': len(self.extended_compounds),
            'corpus_patterns': len(self.bigram_patterns) + len(self.trigram_patterns)
        }


def main():
    """Test hybrid processor"""
    processor = HybridVietnameseProcessor()

    # Test cases tá»« bÃ i toÃ¡n gá»‘c
    test_cases = [
        "toihocbai",           # Core proven: tÃ´i há»c bÃ i
        "toilasinhvien",       # Core proven: tÃ´i lÃ  sinh viÃªn
        "homnaytoilam",        # Core proven: hÃ´m nay tÃ´i lÃ m
        "xemphimhomnay",       # Core proven: xem phim hÃ´m nay
        "dihochomnay",         # Core proven: Ä‘i há»c hÃ´m nay
        "ancomroidi",          # Core proven: Äƒn cÆ¡m rá»“i Ä‘i
        "baitaptoingay",       # Core proven: bÃ i táº­p tá»‘i ngÃ y
        "sinhviennamnhat",     # Core proven: sinh viÃªn nÄƒm nháº¥t
        "maytinh",             # Core proven: mÃ¡y tÃ­nh
        "dienthoai",           # Core proven: Ä‘iá»‡n thoáº¡i
        "vietnamratdep",       # Test extended capabilities
        "hocsinh",             # Test extended
        "giaovien"             # Test extended
    ]

    print("\nğŸ§ª Testing Hybrid Vietnamese Processor:")
    for test_input in test_cases:
        print(f"\nğŸ“ Input: '{test_input}'")
        results = processor.process_text(test_input, max_suggestions=3)

        if results:
            for i, result in enumerate(results, 1):
                icon = {
                    'core_sentence': 'ğŸ¯',
                    'core_compound': 'ğŸ”—',
                    'corpus_trigram': 'â­',
                    'corpus_bigram': 'ğŸ’«',
                    'extended_compound': 'ğŸ“¦',
                    'extended_word': 'ğŸ“š',
                    'segmentation_variation': 'ğŸ§ ',
                    'partial_match': 'ğŸ¤”'
                }.get(result['method'], 'âš¡')

                print(
                    f"  {i}. {result['vietnamese_text']} ({result['confidence']}%) {icon}")
        else:
            print("  âŒ No suggestions found")

    # Statistics
    stats = processor.get_statistics()
    print(f"\nğŸ“Š Hybrid System Statistics:")
    print(f"  Core proven words: {stats['core_count']:,}")
    print(f"  Extended words: {stats['extended_count']:,}")
    print(f"  Total coverage: {stats['total_dictionaries']:,}")


if __name__ == "__main__":
    main()

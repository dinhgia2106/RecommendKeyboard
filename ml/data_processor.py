#!/usr/bin/env python3
"""
Vietnamese Data Processor
Xá»­ lÃ½ Viet74K dictionary vÃ  corpus Ä‘á»ƒ xÃ¢y dá»±ng há»‡ thá»‘ng gá»£i Ã½ nÃ¢ng cao
"""

import re
import json
import unicodedata
from typing import Dict, List, Set, Tuple
from collections import Counter, defaultdict


class VietnameseDataProcessor:
    """Xá»­ lÃ½ vÃ  phÃ¢n loáº¡i dá»¯ liá»‡u tiáº¿ng Viá»‡t tá»« Viet74K vÃ  corpus"""

    def __init__(self, viet74k_path: str = "data/Viet74K.txt", corpus_path: str = "data/corpus-full.txt"):
        self.viet74k_path = viet74k_path
        self.corpus_path = corpus_path

        # PhÃ¢n loáº¡i tá»« vá»±ng
        self.syllables = {}           # Ã‚m tiáº¿t Ä‘Æ¡n
        self.simple_words = {}        # Tá»« Ä‘Æ¡n giáº£n 2-3 Ã¢m tiáº¿t
        self.compound_words = {}      # Tá»« ghÃ©p phá»©c táº¡p
        self.technical_terms = {}     # Thuáº­t ngá»¯ chuyÃªn mÃ´n
        self.place_names = {}         # Äá»‹a danh
        self.person_names = {}        # TÃªn ngÆ°á»i
        self.foreign_words = {}       # Tá»« ngoáº¡i lai

        # Statistics vÃ  patterns
        self.word_frequency = Counter()
        self.ngram_patterns = defaultdict(Counter)
        self.context_patterns = defaultdict(list)

        print("ğŸš€ Vietnamese Data Processor initialized")

    def remove_diacritics(self, text: str) -> str:
        """Loáº¡i bá» dáº¥u tiáº¿ng Viá»‡t"""
        # Vietnamese diacritics mapping
        vietnamese_map = {
            'Ã ': 'a', 'Ã¡': 'a', 'áº¡': 'a', 'áº£': 'a', 'Ã£': 'a',
            'Ã¢': 'a', 'áº§': 'a', 'áº¥': 'a', 'áº­': 'a', 'áº©': 'a', 'áº«': 'a',
            'Äƒ': 'a', 'áº±': 'a', 'áº¯': 'a', 'áº·': 'a', 'áº³': 'a', 'áºµ': 'a',
            'Ã¨': 'e', 'Ã©': 'e', 'áº¹': 'e', 'áº»': 'e', 'áº½': 'e',
            'Ãª': 'e', 'á»': 'e', 'áº¿': 'e', 'á»‡': 'e', 'á»ƒ': 'e', 'á»…': 'e',
            'Ã¬': 'i', 'Ã­': 'i', 'á»‹': 'i', 'á»‰': 'i', 'Ä©': 'i',
            'Ã²': 'o', 'Ã³': 'o', 'á»': 'o', 'á»': 'o', 'Ãµ': 'o',
            'Ã´': 'o', 'á»“': 'o', 'á»‘': 'o', 'á»™': 'o', 'á»•': 'o', 'á»—': 'o',
            'Æ¡': 'o', 'á»': 'o', 'á»›': 'o', 'á»£': 'o', 'á»Ÿ': 'o', 'á»¡': 'o',
            'Ã¹': 'u', 'Ãº': 'u', 'á»¥': 'u', 'á»§': 'u', 'Å©': 'u',
            'Æ°': 'u', 'á»«': 'u', 'á»©': 'u', 'á»±': 'u', 'á»­': 'u', 'á»¯': 'u',
            'á»³': 'y', 'Ã½': 'y', 'á»µ': 'y', 'á»·': 'y', 'á»¹': 'y',
            'Ä‘': 'd',
            # Uppercase
            'Ã€': 'A', 'Ã': 'A', 'áº ': 'A', 'áº¢': 'A', 'Ãƒ': 'A',
            'Ã‚': 'A', 'áº¦': 'A', 'áº¤': 'A', 'áº¬': 'A', 'áº¨': 'A', 'áºª': 'A',
            'Ä‚': 'A', 'áº°': 'A', 'áº®': 'A', 'áº¶': 'A', 'áº²': 'A', 'áº´': 'A',
            'Ãˆ': 'E', 'Ã‰': 'E', 'áº¸': 'E', 'áºº': 'E', 'áº¼': 'E',
            'ÃŠ': 'E', 'á»€': 'E', 'áº¾': 'E', 'á»†': 'E', 'á»‚': 'E', 'á»„': 'E',
            'ÃŒ': 'I', 'Ã': 'I', 'á»Š': 'I', 'á»ˆ': 'I', 'Ä¨': 'I',
            'Ã’': 'O', 'Ã“': 'O', 'á»Œ': 'O', 'á»': 'O', 'Ã•': 'O',
            'Ã”': 'O', 'á»’': 'O', 'á»': 'O', 'á»˜': 'O', 'á»”': 'O', 'á»–': 'O',
            'Æ ': 'O', 'á»œ': 'O', 'á»š': 'O', 'á»¢': 'O', 'á»': 'O', 'á» ': 'O',
            'Ã™': 'U', 'Ãš': 'U', 'á»¤': 'U', 'á»¦': 'U', 'Å¨': 'U',
            'Æ¯': 'U', 'á»ª': 'U', 'á»¨': 'U', 'á»°': 'U', 'á»¬': 'U', 'á»®': 'U',
            'á»²': 'Y', 'Ã': 'Y', 'á»´': 'Y', 'á»¶': 'Y', 'á»¸': 'Y',
            'Ä': 'D'
        }

        result = ""
        for char in text:
            result += vietnamese_map.get(char, char)
        return result.lower()

    def classify_word(self, word: str) -> str:
        """PhÃ¢n loáº¡i tá»« vá»±ng theo category"""
        word = word.strip()
        if not word:
            return "unknown"

        # Check technical terms (cÃ³ dáº¥u gáº¡ch ná»‘i, tá»« ngoáº¡i lai)
        if '-' in word or any(c in word for c in 'xyzwq'):
            return "technical"

        # Check proper names (viáº¿t hoa)
        if word[0].isupper():
            if any(place in word.lower() for place in ['hÃ  ná»™i', 'hcm', 'tp.', 'quáº­n', 'phÆ°á»ng', 'xÃ£']):
                return "place"
            return "name"

        # Check by syllable count
        syllables = word.split()
        if len(syllables) == 1:
            return "syllable"
        elif len(syllables) == 2:
            return "simple_word"
        else:
            return "compound_word"

    def process_viet74k(self) -> Dict:
        """Xá»­ lÃ½ file Viet74K.txt vÃ  phÃ¢n loáº¡i tá»« vá»±ng"""
        print("ğŸ“š Processing Viet74K dictionary...")

        categories = {
            "syllable": [],
            "simple_word": [],
            "compound_word": [],
            "technical": [],
            "place": [],
            "name": [],
            "unknown": []
        }

        try:
            with open(self.viet74k_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    word = line.strip()
                    if not word or len(word) < 2:
                        continue

                    # PhÃ¢n loáº¡i tá»«
                    category = self.classify_word(word)
                    categories[category].append(word)

                    # Táº¡o mapping khÃ´ng dáº¥u -> cÃ³ dáº¥u
                    no_diacritic = self.remove_diacritics(word)
                    if category == "syllable":
                        self.syllables[no_diacritic] = word
                    elif category == "simple_word":
                        self.simple_words[no_diacritic] = word
                    elif category == "compound_word":
                        self.compound_words[no_diacritic] = word
                    elif category == "technical":
                        self.technical_terms[no_diacritic] = word
                    elif category == "place":
                        self.place_names[no_diacritic] = word
                    elif category == "name":
                        self.person_names[no_diacritic] = word

                    if line_num % 10000 == 0:
                        print(f"  Processed {line_num:,} words...")

        except FileNotFoundError:
            print(f"âŒ File not found: {self.viet74k_path}")
            return {}

        # Statistics
        total_words = sum(len(cat) for cat in categories.values())
        print(f"\nğŸ“Š Viet74K Processing Results:")
        print(f"  Total words: {total_words:,}")
        for category, words in categories.items():
            if words:
                print(f"  {category.replace('_', ' ').title()}: {len(words):,}")

        return categories

    def analyze_corpus_sample(self, max_lines: int = 100000) -> Dict:
        """PhÃ¢n tÃ­ch máº«u corpus Ä‘á»ƒ extract patterns"""
        print(f"ğŸ” Analyzing corpus sample ({max_lines:,} lines)...")

        sentence_patterns = []
        word_sequences = []

        try:
            with open(self.corpus_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    if line_num > max_lines:
                        break

                    sentence = line.strip()
                    if len(sentence) < 10:  # Skip short lines
                        continue

                    # Clean sentence
                    sentence = re.sub(r'[^\w\s]', ' ', sentence)
                    words = sentence.split()

                    if len(words) >= 3:
                        sentence_patterns.append(' '.join(words))

                        # Extract word sequences
                        for i in range(len(words) - 1):
                            bigram = f"{words[i]} {words[i+1]}"
                            self.ngram_patterns['bigram'][bigram] += 1

                        for i in range(len(words) - 2):
                            trigram = f"{words[i]} {words[i+1]} {words[i+2]}"
                            self.ngram_patterns['trigram'][trigram] += 1

                    if line_num % 10000 == 0:
                        print(f"  Processed {line_num:,} lines...")

        except FileNotFoundError:
            print(f"âŒ Corpus file not found: {self.corpus_path}")
            return {}

        # Get top patterns
        top_bigrams = self.ngram_patterns['bigram'].most_common(1000)
        top_trigrams = self.ngram_patterns['trigram'].most_common(500)

        print(f"\nğŸ“Š Corpus Analysis Results:")
        print(f"  Bigrams found: {len(self.ngram_patterns['bigram']):,}")
        print(f"  Trigrams found: {len(self.ngram_patterns['trigram']):,}")
        print(f"  Sample patterns:")

        for pattern, count in top_bigrams[:5]:
            print(f"    {pattern} ({count:,})")

        return {
            'bigrams': top_bigrams,
            'trigrams': top_trigrams,
            'total_sentences': len(sentence_patterns)
        }

    def build_enhanced_dictionaries(self) -> Dict:
        """XÃ¢y dá»±ng tá»« Ä‘iá»ƒn nÃ¢ng cao tá»« dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½"""
        print("ğŸ”§ Building enhanced dictionaries...")

        # Combine all dictionaries
        enhanced_dict = {
            'syllables': self.syllables,
            'simple_words': self.simple_words,
            'compound_words': self.compound_words,
            'technical_terms': self.technical_terms,
            'place_names': self.place_names,
            'person_names': self.person_names,
            'ngram_patterns': dict(self.ngram_patterns),
        }

        # Add common sentences from ngrams
        common_sentences = {}
        for trigram, count in self.ngram_patterns['trigram'].most_common(100):
            no_diacritic = self.remove_diacritics(trigram.replace(' ', ''))
            if len(no_diacritic) >= 8:  # Only longer sequences
                common_sentences[no_diacritic] = trigram

        enhanced_dict['common_sentences'] = common_sentences

        # Statistics
        total_entries = sum(len(d)
                            for d in enhanced_dict.values() if isinstance(d, dict))
        print(f"ğŸ“Š Enhanced Dictionary Stats:")
        print(f"  Total entries: {total_entries:,}")
        print(f"  Syllables: {len(self.syllables):,}")
        print(f"  Simple words: {len(self.simple_words):,}")
        print(f"  Compound words: {len(self.compound_words):,}")
        print(f"  Common sentences: {len(common_sentences):,}")

        return enhanced_dict

    def save_processed_data(self, output_path: str = "data/processed_vietnamese_data.json"):
        """LÆ°u dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½"""
        print(f"ğŸ’¾ Saving processed data to {output_path}...")

        # Process all data
        viet74k_data = self.process_viet74k()
        corpus_data = self.analyze_corpus_sample()
        enhanced_dict = self.build_enhanced_dictionaries()

        # Combine all data
        final_data = {
            'metadata': {
                'total_viet74k_words': sum(len(cat) for cat in viet74k_data.values()),
                'total_syllables': len(self.syllables),
                'total_compounds': len(self.compound_words),
                'bigram_patterns': len(self.ngram_patterns['bigram']),
                'trigram_patterns': len(self.ngram_patterns['trigram'])
            },
            'categories': viet74k_data,
            'dictionaries': enhanced_dict,
            'corpus_patterns': corpus_data
        }

        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(final_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… Data saved successfully!")
        except Exception as e:
            print(f"âŒ Error saving data: {e}")

        return final_data

    def quick_test(self):
        """Test nhanh Ä‘á»ƒ kiá»ƒm tra functionality"""
        print("ğŸ§ª Quick test...")

        # Test diacritic removal
        test_words = ["há»c bÃ i", "sinh viÃªn", "HÃ  Ná»™i", "mÃ¡y tÃ­nh"]
        for word in test_words:
            no_diacritic = self.remove_diacritics(word)
            print(f"  {word} â†’ {no_diacritic}")

        # Test classification
        test_classify = ["tÃ´i", "há»c bÃ i",
                         "mÃ¡y tÃ­nh Ä‘iá»‡n tá»­", "Nguyá»…n VÄƒn A", "TP.HCM"]
        for word in test_classify:
            category = self.classify_word(word)
            print(f"  {word} â†’ {category}")


def main():
    """Main function for testing"""
    processor = VietnameseDataProcessor()

    # Quick test
    processor.quick_test()

    # Process and save data
    processed_data = processor.save_processed_data()

    print("\nğŸ‰ Vietnamese Data Processing completed!")
    return processed_data


if __name__ == "__main__":
    main()
